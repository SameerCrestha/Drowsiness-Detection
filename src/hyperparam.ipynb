{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0863c301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb229c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the path to the training and testing XML files\n",
    "TRAIN_PATH = os.path.join(\"ibug_300W_large_face_landmark_dataset\",\"labels_ibug_300W_train_modified.xml\")\n",
    "TEST_PATH = os.path.join(\"ibug_300W_large_face_landmark_dataset\",\"labels_ibug_300W_test_modified.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c5881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the temporary model file\n",
    "TEMP_MODEL_PATH = \"temp.dat\"\n",
    "# define the path to the output CSV file containing the results of\n",
    "# our experiments\n",
    "CSV_PATH = \"trials.csv\"\n",
    "# define the path to the example image we'll be using to evaluate\n",
    "# inference speed using the shape predictor\n",
    "IMAGE_PATH = \"example.jpg\"\n",
    "# define the number of threads/cores we'll be using when trianing our\n",
    "# shape predictor models\n",
    "PROCS = -1\n",
    "# define the maximum number of trials we'll be performing when tuning\n",
    "# our shape predictor hyperparameters\n",
    "MAX_TRIALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c960a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_acc(xmlPath, predPath):\n",
    "# compute and return the error (lower is better) of the shape\n",
    "# predictor over our testing path\n",
    "    return dlib.test_shape_predictor(xmlPath, predPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1905a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_speed(predictor, imagePath, tests=10):\n",
    "# initialize the list of timings\n",
    "    timings = []\n",
    "# loop over the number of speed tests to perform\n",
    "    for i in range(0, tests):\n",
    "        # load the input image and convert it to grayscale\n",
    "        image = cv2.imread(config.IMAGE_PATH)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect faces in the grayscale frame\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        rects = detector(gray, 1)\n",
    "        # ensure at least one face was detected\n",
    "        if len(rects) > 0:\n",
    "            # time how long it takes to perform shape prediction\n",
    "            # using the current shape prediction model\n",
    "            start = time.time()\n",
    "            shape = predictor(gray, rects[0])\n",
    "            end = time.time()\n",
    "        # update our timings list\n",
    "        timings.append(end - start)\n",
    "    # compute and return the average over the timings\n",
    "    return np.average(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bb8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns of our output CSV file\n",
    "cols = [\"tree_depth\",\n",
    "        \"nu\",\n",
    "        \"cascade_depth\",\n",
    "        \"feature_pool_size\",\n",
    "        \"num_test_splits\",\n",
    "        \"oversampling_amount\",\n",
    "        \"oversampling_translation_jitter\",\n",
    "        \"inference_speed\",\n",
    "        \"training_time\",\n",
    "        \"training_error\",\n",
    "        \"testing_error\",\n",
    "        \"model_size\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fea6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV file for writing and then write the columns as the\n",
    "# header of the CSV file\n",
    "csv = open(CSV_PATH, \"w\")\n",
    "csv.write(\"{}\\n\".format(\",\".join(cols)))\n",
    "# determine the number of processes/threads to use\n",
    "procs = multiprocessing.cpu_count()\n",
    "procs = PROCS if PROCS > 0 else procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe3dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of dlib shape predictor hyperparameters that\n",
    "# we'll be tuning over\n",
    "hyperparams = {\n",
    "    \"tree_depth\": list(range(2, 8, 2)),\n",
    "    \"nu\": [0.01, 0.1, 0.25],\n",
    "    \"cascade_depth\": list(range(6, 16, 2)),\n",
    "    \"feature_pool_size\": [100, 250, 500, 750, 1000],\n",
    "    \"num_test_splits\": [20, 100, 300],\n",
    "    \"oversampling_amount\": [1, 20, 40],\n",
    "    \"oversampling_translation_jitter\": [0.0, 0.1, 0.25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d356eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling 10 of 6075 possible combinations\n"
     ]
    }
   ],
   "source": [
    "# construct the set of hyperparameter combinations and randomly\n",
    "# sample them as trying to test *all* of them would be\n",
    "# computationally prohibitive\n",
    "combos = list(ParameterGrid(hyperparams))\n",
    "random.shuffle(combos)\n",
    "sampledCombos = combos[:MAX_TRIALS]\n",
    "print(\"[INFO] sampling {} of {} possible combinations\".format(\n",
    "    len(sampledCombos), len(combos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc22a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over our hyperparameter combinations\n",
    "for (i, p) in enumerate(sampledCombos):\n",
    "    # log experiment number\n",
    "    print(\"[INFO] starting trial {}/{}...\".format(i + 1,len(sampledCombos)))\n",
    "\n",
    "    # grab the default options for dlib's shape predictor and then\n",
    "    # set the values based on our current hyperparameter values\n",
    "    options = dlib.shape_predictor_training_options()\n",
    "    options.tree_depth = p[\"tree_depth\"]\n",
    "    options.nu = p[\"nu\"]\n",
    "    options.cascade_depth = p[\"cascade_depth\"]\n",
    "    options.feature_pool_size = p[\"feature_pool_size\"]\n",
    "    options.num_test_splits = p[\"num_test_splits\"]\n",
    "    options.oversampling_amount = p[\"oversampling_amount\"]\n",
    "    otj = p[\"oversampling_translation_jitter\"]\n",
    "    options.oversampling_translation_jitter = otj\n",
    "    # tell dlib to be verbose when training and utilize our supplied\n",
    "    # number of threads when training\n",
    "    options.be_verbose = True\n",
    "    options.num_threads = procs\n",
    "    # train the model using the current set of hyperparameters\n",
    "    start = time.time()\n",
    "    dlib.train_shape_predictor(TRAIN_PATH,\n",
    "        TEMP_MODEL_PATH, options)\n",
    "    trainingTime = time.time() - start\n",
    "    # evaluate the model on both the training and testing split\n",
    "    trainingError = evaluate_model_acc(TRAIN_PATH,TEMP_MODEL_PATH)\n",
    "    testingError = evaluate_model_acc(TEST_PATH,TEMP_MODEL_PATH)\n",
    "    # compute an approximate inference speed using the trained shape\n",
    "    # predictor\n",
    "    predictor = dlib.shape_predictor(TEMP_MODEL_PATH)\n",
    "    inferenceSpeed = evaluate_model_speed(predictor,IMAGE_PATH)\n",
    "    # determine the model size\n",
    "    modelSize = os.path.getsize(TEMP_MODEL_PATH)\n",
    "# build the row of data that will be written to our CSV file\n",
    "    row = [\n",
    "        p[\"tree_depth\"],\n",
    "        p[\"nu\"],\n",
    "        p[\"cascade_depth\"],\n",
    "        p[\"feature_pool_size\"],\n",
    "        p[\"num_test_splits\"],\n",
    "        p[\"oversampling_amount\"],\n",
    "        p[\"oversampling_translation_jitter\"],\n",
    "        inferenceSpeed,\n",
    "        trainingTime,\n",
    "        trainingError,\n",
    "        testingError,\n",
    "        modelSize,\n",
    "    ]\n",
    "    row = [str(x) for x in row]\n",
    "    # write the output row to our CSV file\n",
    "    csv.write(\"{}\\n\".format(\",\".join(row)))\n",
    "    csv.flush()\n",
    "    # delete the temporary shape predictor model\n",
    "    if os.path.exists(TEMP_MODEL_PATH):\n",
    "        os.remove(TEMP_MODEL_PATH)\n",
    "# close the output CSV file\n",
    "print(\"[INFO] cleaning up...\")\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4630b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
